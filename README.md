# Objective: Setup an MLOps pipeline that allows for continuous improvement and explainability.

Tasks: Use the Fashion MNIST dataset for this exercise.

## M1: Exploratory Data Analysis (EDA)
Objective: Automate data analysis and generate reports for deeper insights.
Tasks:
Use an EDA tool that allows for automated data analysis and report generation (e.g., Pandas Profiling, Sweetviz, or D-Tale).
Provide visual summaries of the dataset, including class distribution, missing values, and feature correlations.
Deliverables:

A report generated by the EDA tool.
Screenshots showcasing key findings.

## M2: Feature Engineering & Explainability
Objective: Build a feature engineering pipeline with explainability visualizations.
Tasks:
Feature Engineering:
Implement preprocessing steps such as normalization, scaling, or transformations.
Explainability:
Use an open-source explainability library (e.g., SHAP, LIME, or InterpretML) to illustrate how each feature affects the class.
Use insights from explainability to refine the feature engineering pipeline.
Deliverables:

Feature engineering pipeline code.
Explainability visualizations and analysis.
Justification of selected features based on explainability results.

## M3: Model Selection & Hyperparameter Optimization
Objective: Identify the best model using AutoML and optimize its hyperparameters.
Tasks:
Model Selection:
Use an open-source AutoML library (e.g., Auto-sklearn, H2O.ai, TPOT) to select the best-performing model.
Hyperparameter Optimization:
Use an open-source hyperparameter optimization tool (e.g., Optuna, Hyperopt, Ray Tune) to fine-tune the selected model.
Deliverables:

AutoML results comparing multiple models.
Hyperparameter tuning logs.
Justification for the chosen model and hyperparameters.

## M4: Model Monitoring & Performance Tracking
Objective: Track and monitor model performance over time.
Tasks:
Use an open-source tracking tool (e.g., MLflow, Neptune.ai, Weights & Biases) to log model performance metrics.
Implement drift detection to identify when retraining is required.
Deliverables:

Performance tracking logs.
Drift detection implementation.
Screenshots showing model performance over multiple runs.
